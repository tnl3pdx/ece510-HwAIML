# Challenge 6, 7, and 8

Challenges 6, 7, and 8 are all related to developing a perceptron, thus they will be contained in the same directory.

In Challenge 6, the goal is to implement a simple neuron that takes in two inputs and uses a sigmoid activation function. The two functions that the neuron needed to learn was NAND and XOR.

In Challenge 7, the goal is to visualize the neuron's line in 2D space and aminate it every step of the weight updated process.

In Challenge 8, the goal is to develop a multi-layer feed-forward perceptron network with backpropagation.

[Log for Challenge 6/7/8](https://docs.google.com/document/d/1fHLvWTa1VuwOcmNT2DHk5nrVDTTodrpD6BzS71KGAKU/edit?usp=sharing)

## Tasks for Challenge 6
    1. Implement a simple neuron (a.k.a. perceptron) with two inputs and a sigmoid activation function.     (DONE)
    2. Use the perceptron learning rule (Google or LLM it) to train the neuron to realize the following binary logic functions:
        a. NAND     (DONE)
        b. XOR      (DONE)

## Tasks for Challenge 7
    1. Visualize the learning process in a 2D-plane by representing the neuron’s “line” that separates the space.   (DONE)
    2. You can turn that in an animated visualization that illustrates every step of the weight updating process as you apply the perceptron rule. (DONE)


## Tasks for Challenge 8
    1. Implement a multi-layer feed-forward perceptron network. The network should have two input neurons, two hidden neurons, and one output neuron.   (DONE)
    2. Implement the backpropagation algorithm to train your network to solve the XOR logical function.     (DONE)

